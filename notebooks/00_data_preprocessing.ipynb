{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5174e420-27f8-45c6-aa07-5a85e1a1d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KORISNIK\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b48184-2f55-4d9b-97eb-0a0ac00980c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2025.7.34 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7c4b42-edf0-4472-b7c8-59a6e049e9cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.3)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\korisnik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.3 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.3 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.2/11.3 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 4.2 MB/s  0:00:02\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ---------------------------------------- 4/4 [seaborn]\n",
      "\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install pandas numpy tqdm matplotlib seaborn scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1fa4842-d14a-4d72-b796-5a1a029a1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  00_data_preprocessing.ipynb\n",
    "\n",
    "# ======================\n",
    "#  1. LIBRARY IMPORTS\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Configure display for better notebook readability\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)  # Silent download\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588d7f81-f067-4137-93d2-199ca9066a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Reviews: 1,094,411 rows | Products: 8,494 rows\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "#  2. DATA LOADING\n",
    "# ======================\n",
    "def load_review_chunks(paths):\n",
    "    \"\"\"\n",
    "    Efficiently loads and concatenates review chunks\n",
    "    Returns: DataFrame with memory optimization\n",
    "    \"\"\"\n",
    "    review_dfs = []\n",
    "    for path in paths:\n",
    "        chunk = pd.read_csv(\n",
    "            path,\n",
    "            dtype={'product_id': 'string', 'review_text': 'string'},\n",
    "            usecols=['product_id', 'review_text', 'rating']\n",
    "        )\n",
    "        review_dfs.append(chunk)\n",
    "    \n",
    "    return pd.concat(review_dfs, ignore_index=True)\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    review_paths = sorted([f\"../data/{f}\" for f in os.listdir(\"../data\") if f.startswith('reviews_')])\n",
    "    reviews_df = load_review_chunks(review_paths)\n",
    "    \n",
    "    products_df = pd.read_csv(\n",
    "        \"../data/product_info.csv\",\n",
    "        dtype={'product_id': 'string', 'brand_name': 'category'}\n",
    "    )\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Reviews: {len(reviews_df):,} rows | Products: {len(products_df):,} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ff8b53-1ca6-4810-88b9-3635fe21acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame: 1,094,411 reviews\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "#  3. DATA MERGING\n",
    "# ======================\n",
    "def merge_datasets(reviews, products):\n",
    "    \"\"\"Merges with memory optimization and column selection\"\"\"\n",
    "    merged = reviews.merge(\n",
    "        products[['product_id', 'brand_name', 'primary_category', 'price_usd', 'loves_count']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "merged_df = merge_datasets(reviews_df, products_df)\n",
    "print(f\"\\nMerged DataFrame: {merged_df.shape[0]:,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f641437b-00ac-401a-9618-87813688560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After cleaning: 1,011,215 reviews remaining\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    0.887204\n",
      "0    0.112796\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KORISNIK\\AppData\\Local\\Temp\\ipykernel_9764\\1213311263.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "#  4. DATA CLEANING (FIXED VERSION)\n",
    "# ======================\n",
    "def clean_data(df):\n",
    "    \"\"\"Comprehensive cleaning pipeline with proper DataFrame handling\"\"\"\n",
    "    # Create a clean copy upfront to avoid chained assignment\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values - drop rows missing critical fields\n",
    "    df = df.dropna(subset=['review_text', 'rating']).copy()\n",
    "    \n",
    "    # Numerical imputation - use .loc to avoid warnings\n",
    "    num_cols = ['price_usd', 'loves_count']\n",
    "    df.loc[:, num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    \n",
    "    # Categorical imputation - handle category dtype properly\n",
    "    cat_cols = ['brand_name', 'primary_category']\n",
    "    for col in cat_cols:\n",
    "        if pd.api.types.is_categorical_dtype(df[col]):\n",
    "            # Add 'Unknown' to existing categories first\n",
    "            df[col] = df[col].cat.add_categories('Unknown')\n",
    "            df.loc[:, col] = df[col].fillna('Unknown')\n",
    "        else:\n",
    "            df.loc[:, col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Create sentiment labels - filter before assignment\n",
    "    sentiment_df = df[df['rating'] != 3].copy()\n",
    "    sentiment_df.loc[:, 'sentiment'] = (sentiment_df['rating'] >= 4).astype(int)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "cleaned_df = clean_data(merged_df)\n",
    "print(f\"\\nAfter cleaning: {cleaned_df.shape[0]:,} reviews remaining\")\n",
    "print(\"Sentiment distribution:\")\n",
    "print(cleaned_df['sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8064dc05-442a-4d61-826d-1bedc2f54b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "#  5. FEATURE ENGINEERING\n",
    "# ======================\n",
    "def engineer_features(df):\n",
    "    \"\"\"Creates analysis-ready features\"\"\"\n",
    "    # Text features\n",
    "    df['review_length'] = df['review_text'].str.len()\n",
    "    df['word_count'] = df['review_text'].str.split().str.len()\n",
    "    \n",
    "    # Product metadata features\n",
    "    df['price_bin'] = pd.qcut(df['price_usd'], q=5, labels=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "enhanced_df = engineer_features(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfe8ac7-8615-4c6d-b63d-f247377d5c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1011215/1011215 [00:15<00:00, 65348.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "#  6. TEXT PREPROCESSING (FIXED VERSION)\n",
    "# ======================\n",
    "from tqdm import tqdm  # Import missing library\n",
    "tqdm.pandas()  # Enable progress_apply\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Robust text cleaning with error handling\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove HTML tags and non-alphabetic characters\n",
    "        text = re.sub(r'<.*?>|[^a-z\\s]', ' ', text)\n",
    "        # Remove stopwords and short words\n",
    "        words = [word for word in text.split() \n",
    "                if word not in stop_words and len(word) > 2]\n",
    "        return ' '.join(words).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply preprocessing with progress bar\n",
    "enhanced_df['clean_text'] = enhanced_df['review_text'].progress_apply(preprocess_text)  # Fixed column name\n",
    "\n",
    "# ======================\n",
    "#  FIXED CATEGORICAL WARNING\n",
    "# ======================\n",
    "def clean_data(df):\n",
    "    \"\"\"Updated to use modern categorical check\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Categorical imputation\n",
    "    cat_cols = ['brand_name', 'primary_category']\n",
    "    for col in cat_cols:\n",
    "        if isinstance(df[col].dtype, pd.CategoricalDtype):  # Modern check\n",
    "            df[col] = df[col].cat.add_categories('Unknown')\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd312e8-42dd-483a-9944-6df887a54f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved full_dataset.parquet using pyarrow\n",
      "✅ Saved product_catalog.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 💾 7. DATA EXPORT (FIXED VERSION)\n",
    "# ======================\n",
    "try:\n",
    "    # Create processed directory if it doesn't exist\n",
    "    os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "    \n",
    "    # Option 1: Try Parquet (faster and more efficient)\n",
    "    try:\n",
    "        enhanced_df.to_parquet(\"../data/processed/full_dataset.parquet\", \n",
    "                             engine='pyarrow',  # Explicitly specify engine\n",
    "                             index=False)\n",
    "        print(\"✅ Saved full_dataset.parquet using pyarrow\")\n",
    "    except ImportError:\n",
    "        # Fallback to CSV if Parquet fails\n",
    "        enhanced_df.to_csv(\"../data/processed/full_dataset.csv\", index=False)\n",
    "        print(\"ℹ️ Saved full_dataset.csv (pyarrow not available)\")\n",
    "    \n",
    "    # Save product catalog (CSV format is fine for this)\n",
    "    product_catalog = enhanced_df[\n",
    "        ['product_id', 'brand_name', 'price_usd', 'primary_category']\n",
    "    ].drop_duplicates()\n",
    "    \n",
    "    product_catalog.to_csv(\"../data/processed/product_catalog.csv\", index=False)\n",
    "    print(\"✅ Saved product_catalog.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Export failed: {str(e)}\")\n",
    "    print(\"Please verify:\")\n",
    "    print(\"1. '../data/processed' directory exists\")\n",
    "    print(\"2. You have write permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cfd624d-a952-44e1-9c66-176dac838541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Data Processing Results\n",
       "\n",
       "### Input Data\n",
       "- **Product Records Loaded:** 8,494\n",
       "- **Review Records Loaded:** 1,094,411\n",
       "- **Merged Dataset:** 1,094,411 reviews\n",
       "\n",
       "### Cleaning Results\n",
       "- **Final Cleaned Reviews:** 1,011,215 \n",
       "- **Sentiment Distribution:**\n",
       "  - Positive (4-5 stars): 88.7%\n",
       "  - Negative (1-2 stars): 11.3%\n",
       "\n",
       "### Output Files\n",
       "1. `full_dataset.parquet` (1,011,215 records)\n",
       "2. `product_catalog.csv` (2,347 unique products)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================\n",
    "#  FINAL OUTPUT FORMATTING\n",
    "# ======================\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Display formatted results\n",
    "Markdown(f\"\"\"\n",
    "## Data Processing Results\n",
    "\n",
    "### Input Data\n",
    "- **Product Records Loaded:** {len(products_df):,}\n",
    "- **Review Records Loaded:** {len(reviews_df):,}\n",
    "- **Merged Dataset:** {len(merged_df):,} reviews\n",
    "\n",
    "### Cleaning Results\n",
    "- **Final Cleaned Reviews:** {len(cleaned_df):,} \n",
    "- **Sentiment Distribution:**\n",
    "  - Positive (4-5 stars): {cleaned_df['sentiment'].mean():.1%}\n",
    "  - Negative (1-2 stars): {1 - cleaned_df['sentiment'].mean():.1%}\n",
    "\n",
    "### Output Files\n",
    "1. `full_dataset.parquet` ({len(enhanced_df):,} records)\n",
    "2. `product_catalog.csv` ({len(enhanced_df[['product_id', 'brand_name', 'price_usd', 'primary_category']].drop_duplicates()):,} unique products)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6d1ff-3fa2-4105-a5a7-b1d466262116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sephora_env)",
   "language": "python",
   "name": "sephora_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
